# -*- coding: utf-8 -*-
"""Telecom_churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fso0XOGF9rp93_8bfXj0gsrsbwQ6PGVi
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
# plt.interactive(True)
# plt.ion()
# matplotlib.is_interactive()
import matplotlib.patches as patches
from matplotlib.path import Path
import sklearn
import warnings
warnings.filterwarnings("ignore")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,roc_auc_score, roc_curve, f1_score, accuracy_score
from sklearn.metrics import make_scorer, precision_score, precision_recall_curve
from sklearn.metrics import recall_score
import warnings
warnings.filterwarnings('ignore')

print(sklearn.__version__)
pd.set_option('display.max_columns',None)
pd.set_option('display.max_rows',None)
pd.set_option("display.precision", 2)
from IPython.display import display

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("telecom_customer_churn.csv")
pd.set_option("display.max_columns", None)
pd.set_option("display.max_colwidth", None)
df.head().style.set_properties(**{'background-color': 'orange',
'color': 'white','border-color': 'darkblack'})

"""**<span style="font-size: 13px;"> We have read dataset using pandas read method. </span>**
**<span style="font-size: 13px;">Now explore the data. </span>**>

**<span style="font-size: 13px;">To explore, process and further investigation on the data, as a Data Analyst, Data Scientist and Machine Learning engineer we must know the meaning and definitation of each and every columns of dataset.** </span>

**<font size="1"> Attributes Definitation** </font>
>

- **As this is the supervised machine learning problem which has lebelled data. So let's first define the independent variables.**
- **CustomerID**: CustomerID define the ID of each individual customer, which should be unique for each.
- **Gender**: Gender define the classification of customer on the basis of gender which values are eithe male or female.
- **Age**: Define the age of customers.
- **Tenure**: Define the time in months that customers have been using internet.
- **MonthlyCharges**: Charges for customers for internet service.
- **TotalCharges**: This is the charge for internet coonnection plus service charge plus monthly fee.
- **Contract**: Define the customers contract for internet service. Once the contrace completed customers has to do repay for service continution.
- **PaymentMethod**: This is the payment channel that customers use to do payment for services.
- **InternetService**: This is the service that customers using for the internet services like optic fiber etc.
- **OnlineSecurity**: Define the customers status for who are using online security of the internet service.
- **OnlineBackup**: Define the customers who are having online backup of their services.
- **DeviceProtection**: Customers who have been using deviceprotection feature of the internet service.
- **TechnicalSupport**: Defines customers who have been taking technical support of the service.
- **StreamingTV**: Customers who are using the feature StreamingTV of the service.
- **StreamingMovies**: Customers who have been using the feature StreamingMovies of the service.-  **Churn: Which is the dependent variable which values are depend on the values of independent variables. Here churn represents the customers who have churned and not churned. so on the basis of independent variable we can predict the customers who are going to leave the service(churn) in the future or near future. We will do that using machine learning algorithms**
"""

df.describe()

"""**The describe() method in pandas is used to generate descriptive statistics that summarize the central tendency, dispersion, and shape of a datasetâ€™s distribution, excluding NaN values by default.**

**For Numerical data it gives the following info.**
- count: The number of non-null entries.
- mean: The mean (average) of the values.
- std: The standard deviation of the values.
- min: The minimum value.
- 25%: The 25th percentile (first quartile).
- 50%: The 50th percentile (median or second quartile).
- 75%: The 75th percentile (third quartile).
- max: The maximum value.

**We can use describe() method for categorical variables also.**
"""

df.describe(include='all')

"""**For categorical variables it provides the following info.**
- count: The number of non-null entries.
- unique: The number of unique values.
- top: The most frequent value (mode).
- freq: The frequency of the top value.

**Now, Identify the duplicate values in data set and removes them and we don't need to keep duplicates data in dataset so that we ensure data integrity and performance.**
"""

duplicates = df.duplicated()
print("Duplicate rows (based on all columns):")
print(df[duplicates])

# Count the number of duplicate rows
num_duplicates = df.duplicated().sum()

print("Number of duplicate rows:", num_duplicates)

"""**There are no any duplicate data in our data set.**

**Now remove the unnecessary variables from our data set. Unnecessary variables are variables which's presence in data set doesnot effect in our business requirement.**

**Once we deeply observed the data we can say CustomerID is not necessary variable in our dataset. So let's remove the variable permanently.**
"""

df.drop(columns=['CustomerID'], inplace=True)

"""**Now let's check the data type of each columns.**"""

df.dtypes

"""**At this point we don't need to change the data type of any column. As all the variables are in correct type.**

**Check the null values in dataset and treat them accordingly to ensure the data completeness**
"""

df.isnull().sum() /100

"""**There is no any null value in any column of the dataset. so we don't need to do anything for null values.**

**Now let's check the unique values of categorical variables,  So that we know the types and uniqueness of values for columns.**

**Let's check the unique values of gender in dataset.**
"""

df.Gender.unique()

"""**Let's check the weight of each unique value in the respective column**"""

count_male = (df['Gender'] == 'Male').sum()
print("Total number of male customers", count_male)

"""**We have 5013 male customers from the 10000 customers.**"""

count_female = (df['Gender'] == 'Female').sum()
print("Total number of female customers", count_female)

"""**We have 4987 female customers from 10000 customers.**

**Now, let's check the uniqueness of customers interms of Contract. and count the weight of each unique value**
"""

df.Contract.unique()

"""**We have four types of customers in the basis of contract. Now let's check in deeply for respective values.**"""

monthly_contract_count = (df["Contract"]=="Monthly").sum()
print("Total customers having monthly contract is :", monthly_contract_count)

"""**We have total 2524 customers from 10000 customers who has monthly contract.**"""

quarterly_contract_count = (df["Contract"]=="Quarterly").sum()
print("Total customers having quarterly contract is :", quarterly_contract_count)

"""**We have total 2518 customers from 10000 customers who has quarterly contract.**"""

semi_annually_contract_count = (df["Contract"]=="Semi Annually").sum()
print("Total customers having semi annually contract is :", semi_annually_contract_count)

"""**We have total 2530 customers from 100000 customers who has semi-annually contract with company**"""

annually_contract_count = (df["Contract"]=="Annually").sum()
print("Total customers having annually contract is :", annually_contract_count)

"""**We have total 2428 customers from 10000 customers who has annually contract with the company**

**There is no such difference in distribution of  customers interms of contract as all values are close.**

**Now let's check theuniqueness of customers interms of PaymentMethod. And count the weight of all the unique value**
"""

df.PaymentMethod.unique()

paywith_credit_card = (df["PaymentMethod"] =="Credit Card").sum()
print("Total customers who has done payment through CREDIT CARD!: ", paywith_credit_card)

"""**We have total 2001 customers who has done paymetn through Credit Card for the internet service.**"""

paywith_cheque = (df["PaymentMethod"] =="Cheque").sum()
print("Total customers who has done payment through CHEQUE!: ", paywith_cheque)

"""**We have total 1950 customers who has done payment through Cheque.**"""

paywith_cash = (df["PaymentMethod"] =="Cash").sum()
print("Total customers who has done CASH payment: !: ", paywith_cash)

"""**We have total 2010 customers who has done payment of service through cash payment.**"""

paywith_bank_transfer = (df["PaymentMethod"] =="Bank Transfer").sum()
print("Total customers who has done payment through bank transfer!: ", paywith_bank_transfer)

"""**We have total 2035 customers who has done payment through bank transfer.**"""

paywith_digital_wallet = (df["PaymentMethod"] =="Digital Wallet").sum()
print("Total customers who has done payment through Digital wallet!: ", paywith_digital_wallet)

"""**We have total 2004 customers who has done service payment through digital wallet.**

**Let's check the uniqueness of customers interms of InternerService**
"""

df.InternetService.unique()

"""**We have total three types of InternetService. Let's check them individually.**"""

fiber_internet_service = (df["InternetService"] == "Fiber optic") . sum()
print("Customers with FIBER OPTIC internet service: ", fiber_internet_service)

"""**We have total 3358 customers who are using fiber optic service.**"""

no_internet_service = (df["InternetService"] == "No").sum()
print("Customers with no internet service: ", no_internet_service)

"""**We have total 3286 customers who are using no service.**"""

dsl_internet_service = (df["InternetService"] == "DSL").sum()
print("Customer having DSL interner service: ", dsl_internet_service)

"""**We have total 3356 customers who are using DSL dervice.**

**Let's check the customers having online security**
"""

customer_having_online_security = (df["OnlineSecurity"] == "Yes").sum()
print("Customers having OnlineSecurity:", customer_having_online_security)
# yes_count = df['OnlineSecurity'].value_counts().get('Yes')
# print(yes_count)

"""**We have total 4922 customers who has online security for the services.**"""

customer_not_having_online_security = (df["OnlineSecurity"] == "No").sum()
print("Customers not having Online Security: ", customer_not_having_online_security)

"""**We have total 5078 customers who has not online security.**"""

customer_having_online_backup = (df["OnlineBackup"] == "Yes").sum()
print("Customer having online backup: ", customer_having_online_backup)

"""**We have total 5089 customers who have online backup.**"""

customer_not_having_online_backup = (df["OnlineBackup"] =="No").sum()
print("Customers not having onlinebackup: ", customer_not_having_online_backup)

"""**We have total 4913 customers who has online backup.**"""

customer_with_device_protection = (df["DeviceProtection"] == "Yes").sum()
print("Customer having device protection: ", customer_with_device_protection)

"""**We have total 5053 customers who are using device protection service.**"""

customer_without_device_protection = (df["DeviceProtection"] == "No").sum()
print("Customers not having device protection:", customer_without_device_protection)

"""**We have total 4947  ustomers who are not using device protection service.**"""

customer_with_tech_support = (df["TechSupport"] == "Yes").sum()
print("Customer who are having tech support:", customer_with_tech_support)

"""**We have total 5031 customers who are using techsupport service.**"""

customer_without_techsupport = (df["TechSupport"] == "No").sum()
print("Customers not having tech support:", customer_without_device_protection)

"""**We have total 4947 customers without tech support service.**"""

having_tvstreaming = (df["StreamingTV"] == "Yes").sum()
print("Customers having TVStreaming connected:", having_tvstreaming)

"""**We have total 4959 customers having StreamingTV service.**"""

not_having_tvstreaming = (df["StreamingTV"] == "No").sum()
print("Customers not connected TVstreaming :", not_having_tvstreaming)

"""**We have total 5055 customers without StreamingTV service.**"""

having_movie_streaming = (df["StreamingMovies"] == "Yes").sum()
print("Customers with movie streaming service:", having_movie_streaming)

"""**We have total 5033 customers having StreamingMovies service.**

---


"""

not_having_moviestreaming = (df["StreamingMovies"] == "No").sum()
print("Customers not having movie streaming service:", not_having_moviestreaming)

"""**We have total 4967 customers who are not having StreamingMovies service.**"""

churn = (df["Churn"] == "Yes").sum()
print("Churned customers:", churn)

not_churn = (df["Churn"] == "No").sum()
print("Customer who didn't churn:", not_churn)

"""**We have almost completed the basic Data Explotation task. Let's move to the Data Visualization and EDA part.**

***Exploratory Data Analysis (EDA) is a method of analyzing datasets to understand their main characteristics. It involves summarizing data features, detecting patterns, and uncovering relationships through visual and statistical techniques. EDA helps in gaining insights and formulating hypotheses for further analysis.***






"""

df.nunique()
(df.isnull().sum()/(len(df)))*100

"""**Before we do EDA, lets separate Numerical and categorical variables for easy analysis.**"""

cat_cols=df.select_dtypes(include=['object']).columns
num_cols = df.select_dtypes(include=np.number).columns.tolist()
print("Categorical Variables:")
print(cat_cols)
print("Numerical Variables:")
print(num_cols)

for col in num_cols:
    print(col)
    print('Skew :', round(df[col].skew(), 2))
    plt.figure(figsize = (15, 4))
    plt.subplot(1, 2, 1)
    df[col].hist(grid=False)
    plt.ylabel('count')
    plt.subplot(1, 2, 2)
    sns.boxplot(x=df[col])
    plt.show()

cat_cols

fig, axes = plt.subplots(2, 2, figsize = (18, 18))
fig.suptitle('Bar plot for all categorical variables in the dataset')

sns.countplot(ax = axes[0, 0], x = 'Gender', data = df, color = 'blue',
              order = df['Gender'].value_counts().index);

sns.countplot(ax = axes[0, 1], x = 'Contract', data = df, color = 'orange',
              order = df['Contract'].value_counts().index);

sns.countplot(ax = axes[1, 0], x = 'PaymentMethod', data = df, color = 'purple',
              order = df['PaymentMethod'].value_counts().index);

sns.countplot(ax = axes[1, 1], x = 'InternetService', data = df, color = 'red',
              order = df['InternetService'].value_counts().index);

# sns.countplot(ax = axes[0, 1], x = 'OnlineSecurity', data = df, color = 'pink',
#               order = df['OnlineSecurity'].value_counts().index);

# sns.countplot(ax = axes[1, 1], x = 'OnlineBackup', data = df, color = 'yellow',
#               order = df['OnlineBackup'].value_counts().index);

# sns.countplot(ax = axes[1, 1], x = 'DeviceProtection', data = df, color = 'green',
#               order = df['DeviceProtection'].value_counts().index);

# sns.countplot(ax = axes[1, 1], x = 'TechSupport', data = df, color = 'black',
#               order = df['TechSupport'].value_counts().index);

# sns.countplot(ax = axes[1, 1], x = 'StreamingTV', data = df, color = 'red',
#               order = df['StreamingTV'].value_counts().index);

# sns.countplot(ax = axes[1, 1], x = 'StreamingMovies', data = df, color = 'pink',
#               order = df['StreamingMovies'].value_counts().index);

# plt.figure(figsize=(6,4))
# sns.countplot(x="Gender", data=df, palette="dark")
# plt.xlabel("Gender", fontsize=10)
# plt.show()

"""**From the above chart we can see the weight of customers in basis of gender.**"""

sns.set(rc={"figure.figsize":(6,4)})
sns.distplot(df["TotalCharges"], kde=True, color="orange", bins=20)

"""**From the above chart we can say that most of the customers paying total charges from 20k to 70k approximately**"""

sns.set(rc={"figure.figsize":(6,4)})
sns.distplot(df["Age"], kde=True, color="orange", bins=20)

"""**From the above chart we can conclude that most of the customer's age falls from 20 to 70**"""

sns.set(rc={"figure.figsize":(6,4)})
sns.distplot(df["MonthlyCharges"], kde=True, color="orange", bins=20)

"""**By seeing the above chart we can conclude that most of the customers paying monthly charge from 2k to 11k.**"""

plt.figure(figsize=(8,6))
sns.boxplot(data=df)
plt.show()

# def remove_outlier(col):
#     sorted(col)
#     Q1,Q3 = col.quantile([0.25,0.75])
#     IQR = Q3 - Q1
#     lower_range = Q1 - (1.5 * IQR)
#     upper_range = Q3 + (1.5 * IQR)
#     return lower_range,upper_range

# lower_range,upper_range = remove_outlier(df["TotalCharges"])
# df["TotalCharges"] = np.where(df["TotalCharges"] > upper_range,
# upper_range, df["TotalCharges"])
# df["TotalCharges"] = np.where(df["TotalCharges"] < lower_range,
# lower_range, df["TotalCharges"])

# plt.figure(figsize=(8,6))
# sns.boxplot(data=df)
# plt.show()

df.nunique().plot(kind='bar')
plt.title('No of unique values in the dataset')
plt.show()

sns.displot(data=df, x="Age", hue="Contract", col="Gender", kind="kde");

sns.displot(data=df, x="Age", hue="PaymentMethod", col="Gender", kind="kde");

sns.kdeplot( data=df, x="Tenure", hue="InternetService", fill=True, common_norm=False, palette="tab10");

contract_count = df.Contract.value_counts()
# Define labels and size:
labels = contract_count.index
sizes = contract_count.values
# Colors for each slice
colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']
# Create the pie chart
plt.figure(figsize=(10, 6))

wedges, texts, autotexts = plt.pie(sizes, labels=labels, colors=colors,
                                   autopct='%1.1f%%', shadow=True, startangle=140,
                                   wedgeprops=dict(edgecolor='w'))

# Customizing text properties
plt.setp(texts, size=10, weight='bold', color='black')
plt.setp(autotexts, size=9, weight='bold', color='white')

# Customizing the legend
plt.legend(wedges, labels, title="Contract Types", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adding a title
plt.title('Distribution of Contract Types', fontsize=14, weight='bold')

# Ensuring the pie chart is circular
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Display the plot
plt.show()

paymentmethod_count = df.PaymentMethod.value_counts()
# Define labels and size:
labels = paymentmethod_count.index
sizes = paymentmethod_count.values
# Colors for each slice
colors = ['#fd9999','#66b3bf','#9ccf99','#ffff99']
# Create the pie chart
plt.figure(figsize=(10, 6))

wedges, texts, autotexts = plt.pie(sizes, labels=labels, colors=colors,
                                   autopct='%1.1f%%', shadow=True, startangle=140,
                                   wedgeprops=dict(edgecolor='w'))

# Customizing text properties
plt.setp(texts, size=10, weight='bold', color='black')
plt.setp(autotexts, size=9, weight='bold', color='white')

# Customizing the legend
plt.legend(wedges, labels, title="Contract Types", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adding a title
plt.title('Distribution of Contract Types', fontsize=14, weight='bold')

# Ensuring the pie chart is circular
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Display the plot
plt.show()

payminternetservice_count = df.InternetService.value_counts()
# Define labels and size:
labels = payminternetservice_count.index
sizes = payminternetservice_count.values
# Colors for each slice
colors = ['#fdff99','#66b3bE','#9fdf99']
# Create the pie chart
plt.figure(figsize=(10, 6))

wedges, texts, autotexts = plt.pie(sizes, labels=labels, colors=colors,
                                   autopct='%1.1f%%', shadow=True, startangle=140,
                                   wedgeprops=dict(edgecolor='w'))

# Customizing text properties
plt.setp(texts, size=10, weight='bold', color='black')
plt.setp(autotexts, size=9, weight='bold', color='white')

# Customizing the legend
plt.legend(wedges, labels, title="Contract Types", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

# Adding a title
plt.title('Distribution of Contract Types', fontsize=14, weight='bold')

# Ensuring the pie chart is circular
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Display the plot
plt.show()

df['OnlineSecurity'].value_counts().plot(kind='pie', autopct='%.2f', labels=df['OnlineSecurity'])

df['OnlineBackup'].value_counts().plot(kind='pie', autopct='%.2f', labels=df['OnlineBackup'])

df['DeviceProtection'].value_counts().plot(kind='pie', autopct='%.2f', labels=df['DeviceProtection'])

df['TechSupport'].value_counts().plot(kind='pie', autopct='%.2f', labels=df['TechSupport'])

df['StreamingTV'].value_counts().plot(kind='pie', autopct='%.2f', labels=df['StreamingTV'])

df['StreamingMovies'].value_counts().plot(kind='pie', autopct='%.2f', labels=df['StreamingMovies'])

df['InternetService'].value_counts()

plt.figure(figsize=(10,5))
df['InternetService'].value_counts().plot(kind='bar')
plt.title('Customer InternetService')
plt.xlabel('Internet Service')
plt.ylabel('Count')
plt.ylim(0, 6000)

df['Contract'].value_counts()

plt.figure(figsize=(10,5))
df['Contract'].value_counts().plot(kind='bar')
plt.title('Customer Contract')
plt.xlabel('Contract Type')
plt.ylabel('Count')
plt.ylim(0, 6000)

df["PaymentMethod"].value_counts()

df['PaymentMethod'].value_counts().plot(kind='bar')
plt.title('Customer Payment Method')
plt.xlabel('Payment Type')
plt.ylabel('Count')
plt.ylim(0, 6000)

df["InternetService"].value_counts()

df['InternetService'].value_counts().plot(kind='bar')
plt.title('Customer Internet Service')
plt.xlabel('Internet Service Type')
plt.ylabel('Count')
plt.ylim(0, 6000)

# cc=df.groupby('Gender') ['Age'].mean()
# df3=pd.DataFrame(cc)
# df3
ga = df.groupby('Gender')['Age'].mean().sort_values(ascending=False)
df3 = pd.DataFrame(ga)
df3

gt = df.groupby('Gender')['Tenure'].mean().sort_values(ascending=False)
df4 = pd.DataFrame(gt)
df4

gm = df.groupby('Gender')['MonthlyCharges'].mean().sort_values(ascending=False)
df5 = pd.DataFrame(gm)
df5

gto = df.groupby('Gender')['TotalCharges'].mean().sort_values(ascending=False)
df6 = pd.DataFrame(gto)
df6

ca = df.groupby('Contract')['Age'].mean().sort_values(ascending=False)
df7 = pd.DataFrame(ca)
df7

ct = df.groupby('Contract')['Tenure'].mean().sort_values(ascending=False)
df8 = pd.DataFrame(ct)
df8

cm = df.groupby('Contract')['MonthlyCharges'].mean().sort_values(ascending=False)
df9 = pd.DataFrame(cm)
df9

cto = df.groupby('Contract')['TotalCharges'].mean().sort_values(ascending=False)
df10 = pd.DataFrame(cto)
df10

pa =df.groupby('PaymentMethod')['Age'].mean().sort_values(ascending=False)
df11 = pd.DataFrame(pa)
df11

pt = df.groupby('PaymentMethod')['Tenure'].mean().sort_values(ascending=False)
df12 = pd.DataFrame(pt)
df12

pm = df.groupby('PaymentMethod')['MonthlyCharges'].mean().sort_values(ascending=False)
df13 = pd.DataFrame(pm)
df13

pto = df.groupby('PaymentMethod')['TotalCharges'].mean().sort_values(ascending=False)
df14 = pd.DataFrame(pto)
df14

ia = df.groupby('InternetService')['Age'].mean().sort_values(ascending=False)
df15 = pd.DataFrame(ia)
df15

it = df.groupby('InternetService')['Tenure'].mean().sort_values(ascending=False)
df16 = pd.DataFrame(it)
df16

im = df.groupby('InternetService')['MonthlyCharges'].mean().sort_values(ascending=False)
df17 = pd.DataFrame(im)
df17

ito = df.groupby('InternetService')['TotalCharges'].mean().sort_values(ascending=False)
df18 = pd.DataFrame(ito)
df18

df.head(2)

num_list = ['Age', 'Tenure', 'MonthlyCharges', 'TotalCharges']
fig = plt.figure(figsize=(15,10))
for i in range(len(num_list)):
    column=num_list[i]
    sub=fig.add_subplot(2,3, i+1)
    sns.boxplot(x='Churn', y=column, data=df, palette='RdYlBu_r')

df.select_dtypes(include='object').nunique()

# from sklearn import preprocessing
# for col in df.select_dtypes(include=['object']).columns:
#     label_encoder=preprocessing.LabelEncoder()
#     label_encoder.fit(df[col].unique())
#     df[col]=label_encoder.transform(df[col])
#     print(f"{col}:{df[col].unique()}")

#to check the normality of the distribution
# plt.rcParams["figure.figsize"] = (9,3)
# plt.xlim([0,100])

plt.subplot(1, 2, 2)
Age = plt.hist(df['Age'], density = True, color = "green")
plt.title("Customer Age")

plt.subplot(1, 2, 2)
Tenure = plt.hist(df['Tenure'], density = True, color = "red")
plt.title("Tenure")

plt.subplot(1, 2, 2)
MonthlyCharges = plt.hist(df['MonthlyCharges'], density = True, color = "orange")
plt.title("Monthly Charges")

plt.subplot(1, 2, 2)
TotalCharges = plt.hist(df['TotalCharges'], density = True, color = "blue")
plt.title("TotalCharges")

# sns.set_theme(style="whitegrid")

# plt.figure(figsize=(10, 6))
# sns.barplot(x='Contract', y='Age', hue='Gender', data=df, palette='muted')

# plt.title('Age by contract and  Gender')
# plt.xlabel('Customers Contract')
# plt.ylabel("Customer's age")
# plt.legend(title ='Gender')
# plt.legend()
# # Display the plot
# plt.show()

# plt.figure(figsize=(6, 4))
# sns.barplot(x='Gender', y='MonthlyCharges', data=df)
# plt.title('Monthly Charges by Gender')
# plt.xlabel('Gender')
# plt.ylabel('MonthlyCharges')
# plt.show()

# plt.figure(figsize=(6, 4))
# sns.barplot(x='Gender', y='Tenure', data=df)
# plt.title('Tenure by Gender')
# plt.xlabel('Gender')
# plt.ylabel('Tenure')
# plt.show()

from scipy import stats

num_cols = df.select_dtypes(include=["int64","float64"])
def plots(num_cols, variable):
    plt.figure(figsize=(15,6))
    plt.subplot(1,2, 2)
    #num_cols[variable].hist()
    sns.distplot(num_cols[variable], kde=True, bins=10)
    plt.title(variable)
    plt.subplot(1, 2, 2)
    stats.probplot(num_cols[variable], dist="norm")
    plt.title(variable)
    plt.show()
for i in num_cols.columns:
    plots(num_cols, i)

"""**Now let's start Feature Engineering part. Here, we don't need to do much more as we just need to convert caterogical feature into numerical features. To accolplish this we will use different type of encoding methods provided by sklearn library. like label encoder, onehotencoder, ordinal encoder, binary encoder arrordingly**"""

df.Contract.value_counts()

# let's convert contract values into numerical.
from sklearn.preprocessing import LabelEncoder

# Initialize the label encoder
le = LabelEncoder()

# Fit and transform the data
df['Contract_encoded'] = le.fit_transform(df['Contract'])
df['Payment_Method'] = le.fit_transform(df['PaymentMethod'])
df['Internet_Service'] = le.fit_transform(df['InternetService'])
df['Gender_encoded'] = le.fit_transform(df['Gender'])

import pandas as pd
# Define the mapping
onlinesecurity_mapping = {'Yes': 1, 'No': 0}
onlinebackup_mapping = {'Yes':1, 'No':0}
deviceprotection_mapping = {'Yes':1, 'No':0}
techsupport_mapping = {'Yes':1, 'No':0}
streamingTV_mapping = {'Yes':1, 'No':0}
streamingmovies_mapping = {'Yes':1, 'No':0}
# Apply the mapping
df['Onlinesecurity_encoded'] = df['OnlineSecurity'].map(onlinesecurity_mapping)
df['Onlinebackup_encoded'] = df['OnlineBackup'].map(onlinebackup_mapping)
df['deviceprotection_encoded'] = df['DeviceProtection'].map(deviceprotection_mapping)
df['techsupport_encoded'] = df['TechSupport'].map(techsupport_mapping)
df['streamingTV_encoded'] = df['StreamingTV'].map(streamingTV_mapping)
df['streamingmovies_encoded'] = df['StreamingMovies'].map(streamingmovies_mapping)

df[["Gender","Gender_encoded"]]

"""**Let's encode the dependent/ target variable also using manual mapping which is effective and easy to apply.**"""

churn_mapping = {'Yes':1, 'No':0}
# Apply the mapping
df['Churned'] = df['Churn'].map(churn_mapping)

df[["Churn","Churned"]]

gc=df.groupby('Gender').sum()['Churned']
df3=pd.DataFrame(gc)
df3

cc=df.groupby('Contract').sum()['Churned']
df4=pd.DataFrame(cc)
df4

pc=df.groupby('PaymentMethod').sum()['Churned']
df5=pd.DataFrame(pc)
df5

ic=df.groupby('InternetService').sum()['Churned']
df6=pd.DataFrame(ic)
df6

pg=df.groupby('Gender').sum()['Internet_Service']
df7=pd.DataFrame(pg)
df7

# Filter for male customers who have churned and have an annual contract
filtered_df = df[(df['Gender'] == 'Male') & (df['Churn'] == 'Yes') & (df['Contract'] == 'Annually')]

count = filtered_df.shape[0]

count

df['Churned'].value_counts()

#we oversample the minority class to balance the label
# from sklearn.utils import resample
# churn_majority=df[(df['Churned']==0)]
# churn_minority=df[(df['Churned']==1)]
# churn_minority_upsampled=resample(churn_minority,
# replace=True,
# n_samples=9999,
# random_state=0)
# churn_upsampled=pd.concat([churn_minority_upsampled, churn_majority])

"""**Now Let's remove the original categorical variable from the data set to prevents redundancy and potential confusion during the modeling process. The encoded numerical values will be used by machine learning algorithms, and the original categorical columns are no longer needed**"""

# List of columns to remove
columns_to_remove = ['Gender', 'Contract','PaymentMethod', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
                    'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Churn']

# Correct way without reassigning
df.drop(columns=columns_to_remove,axis=1, inplace=True)
print(df)  # This will print the DataFrame correctly

df.head(3)

"""**Here, in our dtaset we can see that TotalCharges values is bit confusing as it's not the multiply of MonthlyCharges * Tenure. Example: Let's take o index row, in that row tenure is 17(probabily this is in months) and MonthlyCharges is 6365.68, if we multiply both the value is 108,216.56 but in dataset the value of TotalCharges is 339619. So probabily they have added extra services and physical devices price also. So let's do transform. Let's create a column called OverallCharges by adding MonthlyCharges and TotalCharges, which will removes the 2 variables and and add 1 variable in our datasetwhich will make our datase  little bit simple and easy to process.**


"""

# Create a column OverallCharges by adding the value of MonthlyCharges and TotalCharges.
df['OverallCharges'] = df['MonthlyCharges'] + df['MonthlyCharges']
# This code is not giving expected output. So let's check the data integrity.

df.head(2)

# Strip whitespace from columns
df['MonthlyCharges'] = df['MonthlyCharges'].astype(str).str.strip()
df['TotalCharges'] = df['TotalCharges'].astype(str).str.strip()

# Convert to numeric after stripping
df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Additional Check: Validate each addition
for index, row in df.iterrows():
    expected = row['MonthlyCharges'] + row['TotalCharges']
    actual = row['OverallCharges']
    print(f"Row {index}: {row['MonthlyCharges']} + {row['TotalCharges']} = {actual}, Expected = {expected}, Match = {actual == expected}")

print(df.isna().sum())
print(np.isfinite(df).sum())

df['ComputedOverallCharges'] = df['MonthlyCharges'] + df['TotalCharges']

df.head(6452)

# Drop original columns.
df.drop(columns=['MonthlyCharges','TotalCharges', 'OverallCharges'],axis=1, inplace=True)

df.head(2)

df.columns

# Rename columns using a dictionary
df.rename(columns={'Onlinesecurity_encoded': 'OnlineSecurity',
                        'Onlinebackup_encoded': 'OnlineBackup',
                        'deviceprotection_encoded': 'DeviceProtection',
                        'techsupport_encoded': 'TechSupport',
                        'streamingTV_encoded': 'StreamingTV',
                        'streamingmovies_encoded': 'StreamingMovies',
                        'Contract_encoded': 'Contract',
                        'Payment_Method':'PaymentMethod',
                        'Internet_Service':'InternetService',
                        'Gender_encoded':'Gender'
                        }, inplace=True)

df.head(2)

# Reorder the columns
def reorder_columns(df, column_order):
    return df[column_order]

column_order = ['Gender', 'Age', 'Tenure','ComputedOverallCharges','Contract','PaymentMethod',
                'InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport',
                'StreamingTV', 'StreamingMovies', 'Churned']
df = reorder_columns(df, column_order)

df.head(2)

df["ComputedOverallCharges"].min()

df["ComputedOverallCharges"].max()

df['ComputedOverallCharges'].idxmin()

df.iloc[6450]

# import minmaxscaler and standarscaler from sklearn
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the data
df['ComputedOverallCharges_scaled'] = scaler.fit_transform(df[['ComputedOverallCharges']])

df.head(5)

df["Age"].min()

df["Age"].max()

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the data
df['Age_standardized'] = scaler.fit_transform(df[['Age']])

df.head(2)

# drop columns Age and ComputedOverallCharges
df.drop(columns= ['Age','ComputedOverallCharges'], axis=1, inplace=True)

df.head(2)

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the data
df['Tenure_standardized'] = scaler.fit_transform(df[['Tenure']])

df.head(2)

# Drop column Tenure.
df.drop(columns= ['Tenure'], axis=1, inplace=True)

df.head(2)

df.corr()

# # Create the heatmap
# plt.figure(figsize=(15, 9))
# sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)
# plt.title('Correlation Matrix Heatmap')
# plt.show()

# Create the heatmap
plt.figure(figsize=(20, 9))
sns.heatmap(df.corr(), annot=True, cmap='plasma', center=0)
plt.title('Correlation Matrix Heatmap')
plt.show()

# Create the heatmap
# plt.figure(figsize=(20, 9))
# sns.heatmap(df.corr(), annot=True, cmap='PuBu', center=0)
# plt.title('Correlation Matrix Heatmap')
# plt.show()

# Import required methods from sklearn
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier

# from sklearn.ensemble import RandomForestClassifier

X = df.drop('Churned', axis=1)
y = df['Churned']

# model = RandomForestClassifier()
# model.fit(X, y)

# feature_importances = model.feature_importances_

# feature_importances

# corr_matrix = df.corr()
# corr_with_target = corr_matrix['Churned'].abs().sort_values(ascending=False)

# corr_with_target

# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Initialize and Train Random Forest Model"""

# # Initialize Random Forest classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

"""Evaluate the Model"""

# # Make predictions
y_pred = rf_model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

"""Confusion Matrix and Classification Report"""

# # Confusion matrix
# from sklearn.metrics import classification_report

conf_matrix = confusion_matrix(y_test, y_pred)
print('Confusion Matrix:')
print(conf_matrix)

# Classification report
class_report = classification_report(y_test, y_pred)
print('Classification Report:')
print(class_report)

"""Cross-Validation Implementation"""

# import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Define the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Define the cross-validation strategy
# StratifiedKFold is used for classification tasks to ensure each fold has roughly the same class distribution as the original dataset
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform cross-validation
cv_scores = cross_val_score(rf_model, X, y, cv=cv, scoring='accuracy')

# Print cross-validation scores
print("Cross-Validation Scores:")
print(cv_scores)

# Calculate and print mean accuracy across folds
print(f"Mean Accuracy: {np.mean(cv_scores):.2f}")

"""Initialize and Train Gradient Boosting Model"""

# # Initialize Gradient Boosting classifier
# from sklearn.ensemble import GradientBoostingClassifier
# gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

# # Train the model
# gb_model.fit(X_train, y_train)

"""Predictions and Accuracy"""

# # Make predictions
# y_pred = gb_model.predict(X_test)

# # Calculate accuracy
# accuracy = accuracy_score(y_test, y_pred)
# print(f'Accuracy: {accuracy:.2f}')

"""Confusion Matrix and Classification Report"""

# # Confusion matrix
# conf_matrix = confusion_matrix(y_test, y_pred)
# print('Confusion Matrix:')
# print(conf_matrix)

# # Classification report
# class_report = classification_report(y_test, y_pred)
# print('Classification Report:')
# print(class_report)

"""Feature Importance"""

# # Extract feature importances
# feature_importances = gb_model.feature_importances_

# # Create a DataFrame to display feature importances
# feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})
# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# print('Feature Importances:')
# print(feature_importance_df)

import pickle

# import pickle

# # Load the model
# model_path = 'trained_model.pkl'
# with open(model_path, 'rb') as f:
#     model = pickle.load(f)

# Make pickle file of our model
pickle.dump(rf_model, open("churn.pkl", "wb"))

df.columns

